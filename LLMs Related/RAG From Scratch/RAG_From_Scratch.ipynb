{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def load_pdf(pdf_file_path):\n",
    "    contents = []\n",
    "    doc = fitz.open(pdf_file_path)\n",
    "\n",
    "    for page in doc:\n",
    "        content = page.get_text()\n",
    "        contents.append(\"\\n\" + content)\n",
    "    \n",
    "    return \"\\n\".join(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_pdf(\"C:/Users/vasan/Downloads/resume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVasanth\\nProfile:\\nDetail-oriented and innovative Data Scientist with a specialization in \\nNatural Language Processing (NLP) and Large Language Models (LLM). Excels \\nin leveraging advanced machine learning techniques to derive actionable \\ninsights from textual data. Passionate about pushing the boundaries of NLP \\nto solve real-world problems and drive business value.\\nProfessional Experience:\\nSenior Data Scientist\\nTechGenius AI, Silicon Valley\\nJuly 2020 - Present\\nLed a team of data scientists in developing cutting-edge NLP models, \\nresulting in a X% improvement in accuracy over previous solutions.\\nImplemented sentiment analysis and entity recognition algorithms to \\nextract key insights from customer feedback, driving product improvements.\\nCollaborated with software engineers to deploy NLP models into production \\nsystems, ensuring scalability and real-time performance.\\nConducted research into novel approaches to language modeling, including \\ntransformer-based architectures, to stay at the forefront of NLP \\nadvancements.\\nData Science Intern\\nInnoData Corp, San Francisco\\nMay 2019 - August 2019\\nAssisted in the development of a recommendation system using natural \\nlanguage processing techniques, resulting in a X% increase in user \\nengagement.\\nConducted exploratory data analysis on large-scale text datasets to \\nidentify patterns and trends, informing business strategy.\\nContributed to the development of automated text summarization algorithms, \\nreducing manual effort and improving efficiency.\\nEducation:\\nMaster of Science in Data Science\\nGraduated: May 2020\\nBachelor of Science in Computer Science\\nGraduated: May 2018\\nSkills:\\nProgramming Languages: Python, R, SQL\\nMachine Learning Libraries: TensorFlow, PyTorch, scikit-learn\\nNLP Frameworks: NLTK, spaCy, Transformers\\nData Visualization: Matplotlib, Seaborn, Tableau\\nStatistical Analysis: Hypothesis testing, Regression analysis\\nBig Data Technologies: Hadoop, Spark\\nProjects:\\n\\n\\nSentiment Analysis of Twitter Data\\nDeveloped a sentiment analysis model using deep learning techniques to \\nclassify tweets as positive, negative, or neutral.\\nImplemented a real-time dashboard to visualize sentiment trends over time, \\nproviding valuable insights for marketing campaigns.\\nTopic Modeling of News Articles\\nApplied Latent Dirichlet Allocation (LDA) to identify topics in a large \\ncorpus of news articles.\\nCreated an interactive web application to explore topic distributions and \\nrelated articles, facilitating knowledge discovery.\\nNamed Entity Recognition for Legal Documents\\nBuilt a named entity recognition (NER) model tailored to identify entities \\nrelevant to legal documents, such as laws, regulations, and organizations.\\nDeployed the model as part of a document analysis pipeline, automating the \\nextraction of key information for legal professionals.\\nCertifications:\\nDeep Learning Specialization (Coursera)\\nNatural Language Processing with Deep Learning (Stanford University)\\nLanguages:\\nFluent in English and Spanish\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(text, chunk_size, overlap):\n",
    "\n",
    "    ## Vasanth is saying\n",
    "    ## [Vasan, th is , sayin, g] -> chunk_size=5, overlap=0\n",
    "    ## [Vasan, santh, nth i, h is , is sa, saying] -> chunk_size=5, overlap=3\n",
    "\n",
    "    split_lists = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        split_lists.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return split_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_documents(docs, 500, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nVasanth\\nProfile:\\nDetail-oriented and innovative Data Scientist with a specialization in \\nNatural Language Processing (NLP) and Large Language Models (LLM). Excels \\nin leveraging advanced machine learning techniques to derive actionable \\ninsights from textual data. Passionate about pushing the boundaries of NLP \\nto solve real-world problems and drive business value.\\nProfessional Experience:\\nSenior Data Scientist\\nTechGenius AI, Silicon Valley\\nJuly 2020 - Present\\nLed a team of data scientists in d',\n",
       " ' 2020 - Present\\nLed a team of data scientists in developing cutting-edge NLP models, \\nresulting in a X% improvement in accuracy over previous solutions.\\nImplemented sentiment analysis and entity recognition algorithms to \\nextract key insights from customer feedback, driving product improvements.\\nCollaborated with software engineers to deploy NLP models into production \\nsystems, ensuring scalability and real-time performance.\\nConducted research into novel approaches to language modeling, includin',\n",
       " 'to novel approaches to language modeling, including \\ntransformer-based architectures, to stay at the forefront of NLP \\nadvancements.\\nData Science Intern\\nInnoData Corp, San Francisco\\nMay 2019 - August 2019\\nAssisted in the development of a recommendation system using natural \\nlanguage processing techniques, resulting in a X% increase in user \\nengagement.\\nConducted exploratory data analysis on large-scale text datasets to \\nidentify patterns and trends, informing business strategy.\\nContributed to th',\n",
       " 'ds, informing business strategy.\\nContributed to the development of automated text summarization algorithms, \\nreducing manual effort and improving efficiency.\\nEducation:\\nMaster of Science in Data Science\\nGraduated: May 2020\\nBachelor of Science in Computer Science\\nGraduated: May 2018\\nSkills:\\nProgramming Languages: Python, R, SQL\\nMachine Learning Libraries: TensorFlow, PyTorch, scikit-learn\\nNLP Frameworks: NLTK, spaCy, Transformers\\nData Visualization: Matplotlib, Seaborn, Tableau\\nStatistical Analys',\n",
       " 'n: Matplotlib, Seaborn, Tableau\\nStatistical Analysis: Hypothesis testing, Regression analysis\\nBig Data Technologies: Hadoop, Spark\\nProjects:\\n\\n\\nSentiment Analysis of Twitter Data\\nDeveloped a sentiment analysis model using deep learning techniques to \\nclassify tweets as positive, negative, or neutral.\\nImplemented a real-time dashboard to visualize sentiment trends over time, \\nproviding valuable insights for marketing campaigns.\\nTopic Modeling of News Articles\\nApplied Latent Dirichlet Allocation (L',\n",
       " 'ws Articles\\nApplied Latent Dirichlet Allocation (LDA) to identify topics in a large \\ncorpus of news articles.\\nCreated an interactive web application to explore topic distributions and \\nrelated articles, facilitating knowledge discovery.\\nNamed Entity Recognition for Legal Documents\\nBuilt a named entity recognition (NER) model tailored to identify entities \\nrelevant to legal documents, such as laws, regulations, and organizations.\\nDeployed the model as part of a document analysis pipeline, automat',\n",
       " 'l as part of a document analysis pipeline, automating the \\nextraction of key information for legal professionals.\\nCertifications:\\nDeep Learning Specialization (Coursera)\\nNatural Language Processing with Deep Learning (Stanford University)\\nLanguages:\\nFluent in English and Spanish\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Torch save vectorstore -> Compute embeddings for document, Embedding Dict, Save Dict as Torch file\n",
    "\n",
    "def store_embeddings_as_dict(embeddings_list, text_list):\n",
    "    embeddings_dict = {text: embedding for text, embedding in zip(text_list, embeddings_list)}\n",
    "    return embeddings_dict\n",
    "\n",
    "def save_embeddings_to_pt(embeddings_dict, filename):\n",
    "    torch.save(embeddings_dict, filename)\n",
    "\n",
    "def create_vectorstore(embed_model, chunks):\n",
    "    chunk_embeddings = []\n",
    "    embedder = SentenceTransformer(embed_model)\n",
    "    for chunk in tqdm(chunks, desc=\"Vasanth Embeddings...\"):\n",
    "        chunk_embedding = embedder.encode(chunk, convert_to_tensor=True).to(\"cpu\")\n",
    "        chunk_embeddings.append(chunk_embedding)\n",
    "    embeddings_dict = store_embeddings_as_dict(chunk_embeddings, chunks)\n",
    "    save_embeddings_to_pt(embeddings_dict, \"embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vasanth Embeddings...: 100%|██████████| 7/7 [00:00<00:00, 24.50it/s]\n"
     ]
    }
   ],
   "source": [
    "create_vectorstore(\"sentence-transformers/all-MiniLM-L6-v2\", chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve top k docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "def retrieve_relevant_docs(embed_model, embeddings_path, query, top_k):\n",
    "    embedder = SentenceTransformer(embed_model)\n",
    "    embeddings_dict = torch.load(embeddings_path)\n",
    "    chunks = list(embeddings_dict.keys())\n",
    "    query_encoded = embedder.encode(query, convert_to_tensor=True)\n",
    "    top_k = min(top_k, len(embeddings_dict))\n",
    "    scores = []\n",
    "    for chunk in tqdm(chunks, desc=\"Computing Similarity...\"):\n",
    "        cos_score = util.cos_sim(query_encoded, embeddings_dict[chunk])[0]\n",
    "        scores.append(cos_score)\n",
    "    scores = torch.Tensor(scores)\n",
    "    top_k_chunk_indices = torch.topk(scores, k=top_k).indices.tolist()\n",
    "    top_k_chunks =[chunks[i] for i in top_k_chunk_indices]\n",
    "    return top_k_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarity...: 100%|██████████| 7/7 [00:00<00:00, 6992.17it/s]\n"
     ]
    }
   ],
   "source": [
    "top_k_docs = retrieve_relevant_docs(\"sentence-transformers/all-MiniLM-L6-v2\", \"embeddings.pt\", \"What are the skills specified\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l as part of a document analysis pipeline, automating the \\nextraction of key information for legal professionals.\\nCertifications:\\nDeep Learning Specialization (Coursera)\\nNatural Language Processing with Deep Learning (Stanford University)\\nLanguages:\\nFluent in English and Spanish\\n',\n",
       " '\\nVasanth\\nProfile:\\nDetail-oriented and innovative Data Scientist with a specialization in \\nNatural Language Processing (NLP) and Large Language Models (LLM). Excels \\nin leveraging advanced machine learning techniques to derive actionable \\ninsights from textual data. Passionate about pushing the boundaries of NLP \\nto solve real-world problems and drive business value.\\nProfessional Experience:\\nSenior Data Scientist\\nTechGenius AI, Silicon Valley\\nJuly 2020 - Present\\nLed a team of data scientists in d',\n",
       " 'ds, informing business strategy.\\nContributed to the development of automated text summarization algorithms, \\nreducing manual effort and improving efficiency.\\nEducation:\\nMaster of Science in Data Science\\nGraduated: May 2020\\nBachelor of Science in Computer Science\\nGraduated: May 2018\\nSkills:\\nProgramming Languages: Python, R, SQL\\nMachine Learning Libraries: TensorFlow, PyTorch, scikit-learn\\nNLP Frameworks: NLTK, spaCy, Transformers\\nData Visualization: Matplotlib, Seaborn, Tableau\\nStatistical Analys']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stuff_docs(relevant_chunks):\n",
    "    relevant_chunks = [f\"Document{i}: {value}\\n\\n\" for i, value in enumerate(relevant_chunks, start=1)]\n",
    "    stuffed_chunk = \"\".join(relevant_chunks)\n",
    "    return stuffed_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document1: l as part of a document analysis pipeline, automating the \\nextraction of key information for legal professionals.\\nCertifications:\\nDeep Learning Specialization (Coursera)\\nNatural Language Processing with Deep Learning (Stanford University)\\nLanguages:\\nFluent in English and Spanish\\n\\n\\nDocument2: \\nVasanth\\nProfile:\\nDetail-oriented and innovative Data Scientist with a specialization in \\nNatural Language Processing (NLP) and Large Language Models (LLM). Excels \\nin leveraging advanced machine learning techniques to derive actionable \\ninsights from textual data. Passionate about pushing the boundaries of NLP \\nto solve real-world problems and drive business value.\\nProfessional Experience:\\nSenior Data Scientist\\nTechGenius AI, Silicon Valley\\nJuly 2020 - Present\\nLed a team of data scientists in d\\n\\nDocument3: ds, informing business strategy.\\nContributed to the development of automated text summarization algorithms, \\nreducing manual effort and improving efficiency.\\nEducation:\\nMaster of Science in Data Science\\nGraduated: May 2020\\nBachelor of Science in Computer Science\\nGraduated: May 2018\\nSkills:\\nProgramming Languages: Python, R, SQL\\nMachine Learning Libraries: TensorFlow, PyTorch, scikit-learn\\nNLP Frameworks: NLTK, spaCy, Transformers\\nData Visualization: Matplotlib, Seaborn, Tableau\\nStatistical Analys\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuffed_doc = stuff_docs(top_k_docs)\n",
    "stuffed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the given context alone.\n",
    "context: {context}\n",
    "question: {question}\n",
    "answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "def qa(llm_name, context, question):\n",
    "    llm_inp = prompt.format(context=context, question=question)\n",
    "    client = Groq(api_key=\"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\")\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": llm_inp,\n",
    "        }\n",
    "    ],\n",
    "    model=llm_name\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the context, the skills specified are:\\n\\n* Programming Languages: Python, R, SQL\\n* Machine Learning Libraries: TensorFlow, PyTorch, scikit-learn\\n* NLP Frameworks: NLTK, spaCy, Transformers\\n* Data Visualization: Matplotlib, Seaborn, Tableau'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"llama3-70b-8192\", top_k_docs, \"What are the skills specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "markdown-validation-crew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
